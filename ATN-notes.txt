I have also started implmemting the ATN experiment and have some initial experiments on the cluseter and colab.
- So far I have settled on a context window of 512 tokens.
- batch size of 16
- 1 epoch per task
- 16*32 = 512 articles from each class for each task
- each task is a binary classification problem 
- Tuning the learning rate, we see some plasticity loss for (L3, 1e-3)
- The network was doing very well with more epochs, so I decided to decrease the number of epochs to 1, making the problem more difficult.


01/23/25
- Vivek suggested that I use more articles, so I am increasing the number of articles by at least a factor of 10.
- Therefore, I have filtered the dataset into each publication and I have tokenized each title+text into 512 token long exaples.
- Many of the publications have on average about 500 tokens of text in the article, therefore, I limit my tokenization to 512 tokens.

Current TODOs:
- Implement new experimental loop using .hf files for each publication.
- Run this experiment on Colab to get a feel for the number of articles per task, epochs, etc.
- Then schedule a job on the cluster to run a learning rate sweep.

Further TODOs:
- Log where we have all of our results, files, etc.
- Do a better job for the ATN experiments.

Some observations:
After preprocessing, here the number of articles for each publication
Vox: 47265
Business Insider: 57934
Reuters: 825136
TMZ: 49504
Vice: 100986
Vice News: 15539
Hyperallergic: 13539
TechCrunch: 52051
Axios: 47311
Refinery 29: 87111
The Verge: 51556
Mashable: 94107
People: 135691
Economist: 23200
CNN: 125300
Gizmodo: 27226
New Yorker: 4644
Wired: 20185
CNBC: 234139
New Republic: 11807
Fox News: 20144
The Hill: 208411
Politico: 46235
The New York Times: 249072
Buzzfeed News: 32724
Washington Post: 3332

- For my experiments, if I want to run with 5000 and more articles per class, I should just drop Washington Post and New Yorker. 
- Then the minimum number of articles per publication becomes: New Republic with 11807 articles.

- For now, I will just get an experiment implemented, schedule the job on the cluster and then I can optimize it later.

01/24/25:
- My job last night did not run because I forgot to specify the number of layers
- My learning rate sweep is currently running. There are two things that I will be watching for:
- 1: Is plasticity loss occuring.
- 2: How long is the experiment taking.

- Ideas for changes:
- Perhaps I could accelerate the experiment if I do not add the labels ot the dataset, and instead always sample 8 examples from each class at a time.
- Should I increase batch size, like I had in continual imagenet? 
- Perhaps I do a multiclass classification problem, rather than binary. 

- I have updated my experimental files in the evening, but when I try ot pull my git repo in the cluster, I get an out of disk space error, this is quite strange
- I do not know where the excessive data was saved
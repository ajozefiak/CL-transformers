12/23/24

- Here I am creating a notes section.
- Some of this will be moved to a .tex file on Overleaf for ease of presentation and eventual merging into my paper.
- For now I will work with the CI-R1 experiment. The Contiual ImageNet experiment with 1 pass though the dataset.
- I have implemented this experiment as CI_ViT_V1.py and CI_ViT_V1_resets.py
- So far, all config files are in config/December24 Configs/ViT

TODO META: 
- I need to figure out how and where to log results.
- Master Directory:
- /pool001/jozefiak/CI_ViT/
- ../experiment_name/alg_h-params/seed_i/
- /lr_sweep/ 
- /test_acc.pkl
- /train_acc.pkl
- /train_loss.pkl
- e.g. - /pool001/jozefiak/CI_ViT/lr_sweep/Vanilla_lr_1e-1/seed_0/test_acc.pkl
- TODO: make a function that saves to disk and creates the necessary directory if it does not experiments


TODO 1:
- I will begin by performing a learning rate sweep for 5 seeds.
- I should record the train loss, train accuracy, and train error.
- I could record other covarites, but perhaps that should be done later.

- I have completed TODO 1.
- For 3 layers, lr = 1e-3 is better initially and has clear plasticity loss while lr = 1e-4 performs better eventually.
- For 12 layers, lr = 1e-4 is clearly better htroughout and shows minimal plasticity train_loss
- For 12 layers, lr = 1e-3 shows clear plasticity loss and is better than lr = 1e-5 initially, which seems to improve over time.

TODO 2: I am now implementing and running the hyperparameter sweeps.
- I am running the sweeps for L2 and L2Init from 1e-1 to 1e-7
- I have just launched a Shrink and Perturb sweep. 
- These leaves me with implementing the reset based algorithms, then the *-algorithms.

TODO 3: Reset Algorithms:
- I have made some progress on refactoring my code for the reset algorithms.
- I need to write a new experiment loop with resets.


- Note: For S&P I did not fix the use of the random number generator and I was using the same key throughout an epoch.
- This may have caused some issues, so I should look into it later potentially. But there are 500*10 epochs, so perhaps this is not too concerning for now.

12/30/24 Update: I am back from my trip. I will begin today with implementing and testing the reset algorithms. Then I will run the massive hyperparameter sweep.
- CI_ViT_V1_resets.py looks good, but I need to run it to be sure.
- get_neuron_ages_ViT.py looks reasonable, but I should double check that this one works.
- reset_neurons_ViT.py seems to look correct for CBP, though I really need to test it

- TODO: I just need to change the configs to account for the number of neurons due to scaling: n_neurons = 4 * hidden_dim


12/23/24

- Here I am creating a notes section.
- Some of this will be moved to a .tex file on Overleaf for ease of presentation and eventual merging into my paper.
- For now I will work with the CI-R1 experiment. The Contiual ImageNet experiment with 1 pass though the dataset.
- I have implemented this experiment as CI_ViT_V1.py and CI_ViT_V1_resets.py
- So far, all config files are in config/December24 Configs/ViT

TODO META: 
- I need to figure out how and where to log results.
- Master Directory:
- /pool001/jozefiak/CI_ViT/
- ../experiment_name/alg_h-params/seed_i/
- /lr_sweep/ 
- /test_acc.pkl
- /train_acc.pkl
- /train_loss.pkl
- e.g. - /pool001/jozefiak/CI_ViT/lr_sweep/Vanilla_lr_1e-1/seed_0/test_acc.pkl
- TODO: make a function that saves to disk and creates the necessary directory if it does not experiments


TODO 1:
- I will begin by performing a learning rate sweep for 5 seeds.
- I should record the train loss, train accuracy, and train error.
- I could record other covarites, but perhaps that should be done later.
- The other covariates to record are: neuron activities, neuron ages, entropies of self attention layers, weight magnitudes

- I have completed TODO 1.
- For 3 layers, lr = 1e-3 is better initially and has clear plasticity loss while lr = 1e-4 performs better eventually.
- For 12 layers, lr = 1e-4 is clearly better htroughout and shows minimal plasticity train_loss
- For 12 layers, lr = 1e-3 shows clear plasticity loss and is better than lr = 1e-5 initially, which seems to improve over time.

TODO 2: I am now implementing and running the hyperparameter sweeps.
- I am running the sweeps for L2 and L2Init from 1e-1 to 1e-7
- I have just launched a Shrink and Perturb sweep. 
- These leaves me with implementing the reset based algorithms, then the *-algorithms.

TODO 3: Reset Algorithms:
- I have made some progress on refactoring my code for the reset algorithms.
- I need to write a new experiment loop with resets.


- Note: For S&P I did not fix the use of the random number generator and I was using the same key throughout an epoch.
- This may have caused some issues, so I should look into it later potentially. But there are 500*10 epochs, so perhaps this is not too concerning for now.

12/30/24 Update: I am back from my trip. I will begin today with implementing and testing the reset algorithms. Then I will run the massive hyperparameter sweep.
- CI_ViT_V1_resets.py looks good, but I need to run it to be sure.
- get_neuron_ages_ViT.py looks reasonable, but I should double check that this one works.
-- update_neuron_ages, get_neuron_pre_activ CI_ViT_V1_resets appear to be working.
- reset_neurons_ViT.py seems to look correct for CBP, though I really need to test it
-- neuron_ages do not appear to be updating correctly, or at all rather from the reset_neurons function for CBP
-- Actually, it appears that we do not update neuron_ages in CBP

NOTE: So far my crude tests seem to show that CBP is working as expected, I just need to keep in mind the below notes

Update at 6:20 PM: I have just my CBP sweep for num_layers=3, lr=1e-3, 
- The script is not saving results after each seed like with S&P, which is a bit strange.
- This terminated due a typo, I was using x instead of x_batch, but the warning that I am receiving should be independent of this.
- I am in the process of implementing ReDO and I will try and run ReDO with num_layers=3, lr=1e-3 before running the num_layers=12 experiments.
- The cluster is now much busier.
- ReDO looks good to me, but I have not tested it on the Colab yet.
- I have placed CBP and ReDO jobs on the cluster. They are waiting for priority and resoures.


Update at 6:20 PM of 12/31/24:
- I have ran the CBP sweep, the ReDO sweep has completed for L3 and is in progress for L12
- I have scheduled the SNR sweep. 
- Next, I need to write an experimental loop for running each optimal hyperparameter choice for 5 new seeds and recording the covariates.
- I also need to record the optimal hyperparameters.
- All of this needs to be put into a document, which will be a bit annoying since I need to record all of the experimental details, architectural details and so forth.

Covariates and statistics to record:
- train loss, train accuracy, test accuracy: We are currently doing this.
- Magnitudes of weights: I can record this at the end of each opoch (or task)
- Neuron ages (measures inactivity)
- Neuron resets
- entropies of self attention layers.

- I should implement this and run it for the Vanilla algorithm in Colab to confirm that it works.
- Then I can run this on the cluster or even Colab for every algorithm.


FUTURE TODOs:

- DONE: TODO: I just need to change the configs to account for the number of neurons due to scaling: n_neurons = 4 * hidden_dim
- TODO: For CBP, I may need to lower the age threshold and the perhaps play with the decay factor.
- At the moment, since we are counting inactivity in terms of units of batches, an age threshold of 100 implies that we 
- can only reset a neuron after 100 batches which is almost one task. Perhaps this is not too bad for this application.
- DONE (see below): TODO: For CBP, it does not appear that we are updating neuron_ages in the reset_neurons, therefore:
- DONE: for all methods except SNR, we should be manually updating neuron_ages when logging all covariates. 

1/1/25: 
- Yesterday I have run the SNR algorithms for L3 and L12. The L12 sweeps are still running.
- I have also implemented the CI_ViT_V1_log_correlates.py file which logs all of the above correlates. 
- I have run this for Vanilla L3 with lr 1e-3, and L12 with lr 1e-3 and 1e-4
- I need to now make a notebook on the cluster that goes through each hyperparamter sweep and determines the optimal hyperparameters for each algorithm.

- L2: L3: reg_str = 1e-4, L12 w/ lr = 1e-3: reg_str = 1e-3, L12 w/ lr = 1e-4: reg_str = 1e-5
- L2Init: L3: reg_str = 1e-2, L12 w/ lr = 1e-3: reg_str = 1e-4, L12 w/ lr = 1e-4: reg_str = 1e-2 (last 50) 1e-1 (last 10)

Note S&P results look quite poor for some reason, I may need to debug and figure out what is going on
- S&P: L3  w/ lr = 1e-3: p = 0.99, sigma = 1e-2
- S&P: L12 w/ lr = 1e-3: p = 0.7,  sigma = 1e-1 (last 50), p = 0.99, sigma = 1e-1 (last 10)
- S&P: L12 w/ lr = 1e-4: p = 0.99, sigma = 1e-2, (last 50) p = 0.975, sigma = 1e-2 (last 10)

- ReDO: L3  w/ lr = 1e-3: threshold = 0.08, reset_freq = 0.0166... (last 50), threshold = 0.02, reset_freq = 0.0166... (last 10)
- ReDO: L12 w/ lr = 1e-3: threshold = 0.01, reset_freq = 0.0166... (last 50), threshold = 0.00, reset_freq = 0.002083... (last 10)
- ReDO: L12 w/ lr = 1e-4: threshold = 0.08, reset_freq = 0.0166... (last 50), threshold = 0.08, reset_freq = 0.0166... (last 10)

- CBP: L3: reset_freq = 1e-2, L12 w/ lr = 1e-3: reset_freq = 1e-2, L12 w/ lr = 1e-4: reset_freq = 1e-2 (last 50) 1e-1 (last 10, barely better)

- SNR: 
L3 lr=1e-3:
best_reset_percentile: 0.84, best_reset_freq: 0.004166666666666667, terminal 50 task accuracies: 0.6416800022125244
best_reset_percentile: 0.96, best_reset_freq: 0.004166666666666667, terminal 10 task accuracies: 0.6053999662399292

L12 lr=1e-3:
best_reset_percentile: 0.99, best_reset_freq: 0.0020833333333333333, terminal 50 task accuracies: 0.6072399616241455
best_reset_percentile: 0.96, best_reset_freq: 0.0005208333333333333, terminal 10 task accuracies: 0.5743999481201172

L12 lr=1e-4:
best_reset_percentile: 0.98, best_reset_freq: 0.0020833333333333333, terminal 50 task accuracies: 0.737280011177063
best_reset_percentile: 0.68, best_reset_freq: 0.0020833333333333333, terminal 10 task accuracies: 0.7029999494552612

Midnight of 1/1/31:

- I realize that when choosing the optimal hyperparameter, I based my choices off the optimal test accuracy over the final 50 epochs,
- rather than the terminal epoch's test accuracy over the last 50 tasks => so this may require rerunning everything.
- Results, for L3 so far, look good for everything except SNR.
- If the issue isn't with the implementation and the hyperparameter sweep, below are some thoughts, but first...
- Tomorrow, I should spend some time with 1) either quickly setting up experiments or summarizing my work so far
- Then 2) I should implement the new experiments for SNR

Here are the thoughts that I jotted down in a notebook on the cluster when analyzing intial results for L3:
# If the SNR implementation is correct, then I suspect two things:
# 1) Having a large batch size may be causing, some, discrepencies with the earlier results. 
# Perhas ReDO and CBP can compute more accurate utilities with larger batch sizes and SNR is too late in detecting inactive neurons
# Here, a batch sonsists of 100 images * 65 tokens/image = 6500 neuron firings per batch
# This is much larger than the earlier MLP and CNN experiments which had fewer neuron firings per batch
# 2) It may just be that ReDO and CBP fare better at estimating a neuron's 'utliity'
# 3) It is also strange that SNR does worse than Vanilla
# 4) SNR has 36-61X fewer resets than CBP and ReDO over the course of the experiment
# 5) CBP and ReDO appear to have more dormant neurons, which means that they are also resetting other neurons


# I should record my results, somewhere, and simultaneously, I should debug my SNR implementation.
# Currently, the fact that SNR has fewer inactive neurons matches the expectations. 

# Other note, I find it quite strange that SNR does worse than the Vanilla algorithm,
# it's almost as though resetting the inactive neurons is worse than not resetting and also worse than overly resetting

# I have two ideas for dealing with SNR in this regime:
# - allow for \epsilon-death which means that a neuron is inactive if it does not fire for sufficiently many 'inputs'
# - keep track of inactivity in some continuous manner, so if the neuron does not fire for 80% of neurons
# then treat it as not firing for 0.8 * batch_size * # of tokens
# I need to think about this and perhaps probe Chat-GPT.
# One "greedy" approach is to assume that that the inactivities are continuguous and either arrive first,
# or arrive last.
# Arrive first => compound with previous steps inactivity, but this may just accumulate
# Arrive last => No accumulation, so neuron inactivity resets with each batch => probably the right thing to do.

# All I can say about SNR vs Vanilla is: if you're going to reset neurons, you better reset early enough otherwise you are potenttialy losing out on forward transfer with delayed plasticity gain.

01/03/25

Hyperparameter Sweep Correctness:
- I had chosen hyperparameters based off the test accuracy on the last 50 epochs rather than the last 50 terminal task accuracies.
- Here I am writing down each algorithm and it's hyperparameters and whether it needs to be rerun.

L3: 
Vanilla: N/A
L2: (OKAY~) ran: reg_str = 1e-4, correct: reg_str = 1e-3 (slightly better)
L2Init: (Perfect) ran: reg_str = 1e-2, correct: reg_str = 1e-2
S&P: (Perfect) ran/correct: p = 0.99, sigma = 0.01
CBP: (OKAY~) ran: reset_freq = 1e-2, correct: reset_freq = 1e-1 (slightly better at @ terminal 50 tasks, but worse at terminal 10 tasks)
ReDO: (Perfect) ran/correct: threshold = 0.08, reset_freq = 1 / (0.5 * batches * epochs)
SNR: (OKAY~) ran: reset_percentile = 0.84, reset_freq = 1 / (2 * batches * epochs), correct: reset_percentile = 0.84, reset_freq = 1 / (4 * batches * epochs) (h_params are similar and results are both poor for SNR)
SNR-V2: (Perfect) ran/correct/finer-sweep: reset_percentile = 0.55, reset_freq = 1 / (0.75 * batches * epochs)

L12 w/ lr = 1e-3: 
Vanilla: N/A
L2: (OKAY~) ran: reg_str = 1e-3, correct: reg_str = 1e-4 (slightly better)
L2Init: (OKAY~) ran: reg_str = 1e-4, correct: reg_str = 1e-2 (better by at most 0.01)
S&P: (Perfect): ran/correct: p = 0.7, sigma = 0.1
CBP: (Perfect) ran: reset_freq = 1e-2, correct: reset_freq = 1e-2
ReDO: (Maybe rerun~) ran: threshold = 0.01, reset_freq = 1 / (0.5 * batches * epochs), correct: threshold = 0.02, reset_freq = 1 / (0.25 * batches * epochs) (the performance looks better for the correct one, but all of these look pretty bad relative to Vanilla)
SNR: (OKAY~) ran: reset_percentile = 0.99, reset_freq = 1 / (4 * batches * epochs), correct: reset_percentile = 0.92, reset_freq = 1 / (4 * batches * epochs) (H_params are similar and results are both poor for SNR)
SNR-V2: (TODO) correct/original-sweep: reset_percentile = 0.6, reset_freq = 1 / (8 * batches * epochs) (Currently running the finer/wider sweep)

L12 w/ lr = 1e-4: 
Vanilla: N/A
L2: (Perfect) ran/correct: reg_str = 1e-5
L2Init: (Perfect): ran/correct: reg_str = 1e-2
S&P: ran: (Perfect): ran/correct p = 0.99, sigma = 1e-2
CBP: (OKAY~) ran: reset_freq = 1e-2, correct: reset_freq = 1e-1 (very, very similar performance)
ReDO: (Maybe rerun~) ran: threshold = 0.08, reset_freq = 1 / (0.5 * batches * epochs) correct: threshold = 0.04, reset_freq = 1 / (1 * batches * epochs) (the correct one is about 0.01 better than what I ran, h_params are similar, but it could merit rerunning this)
SNR: (Maybe/OKAY~) ran: reset_percentile = 0.98, reset_freq = 1 / (4 * batches * epochs), correct: reset_percentile = 0.98, reset_freq = 1 / (32 * batches * epochs) (similar h_params, and performance is only slightly better for the correct h_params, could be noise)
SNR-V2: (TODO) correct/original-sweep: reset_percentile = 0.6, reset_freq = 1 / (1 * batches * epochs)

- I have just setup an finer sweep for SNR-V2 w/ L12 and lr = 1e-3 that is slightly wider than the finer sweep for SNR-V2 L3.
- I am just extending the reset_freq up to 15 and 32, given the original hyperparameter sweep results.
- I am going to run the finer hyperparamter sweep for L!2 w/ lr = 1e-4, but will keep the original range of reset_freqs as in the finer L3 sweep

- It looks like there is no algorithm that needs its optimal hyperparameter choice to be really 

01/07/25:

- I have not yet analyzed the optimal hypermaters for SNR-V2 in the twelve layer experiments yet, though I should do that and potentially rerun those
- I need to run this later

- Vivek was also asking about SNR-L2, therefore I need to implement:
- SNR-L2
- SNR-V2-L2
- SNR-L2*
- SNR-V2-L2*

In the mean time, I can run:
- SNR-L2 (3 layer, 12 layers)
- SNR-V2-L2 (3 layers) 

- The question is, should I do a hyperparameter sweep, perhaps I do that after I get my results.
- I should also log all of my results into my document. This includes the learning rate sweep, the hyperparameter sweep, and the choice of hyperparameters.

- Before running any of this, I have used 898 GB of my pool001 directory. 
- A layer logging experiment consumes 13 GB and a 3 layer experiminet conumes 900 Maybe

- My immediate TODOs are to:
- impolement SNR-L2, SNR-V2-L2
- run: SNR-L2, SNR-V2-L2, and L2 with the correct optimal hyperparameters. This will give suboptimal results for the SNR variants, which I can tune later.

Intermediate TODOs:
- I should run the L3 experiment in a notebook and I should record the thresholds. Are these absurdly large? 
- I should double check the derivation of my SNR-V2 reset technique

Tomorrow, or later: 
- I can run a more exhaustive hyperparameter sweep.
- I can implement L2*, SNR-L2*, SNR-V2-L2*
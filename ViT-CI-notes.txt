12/23/24

- Here I am creating a notes section.
- Some of this will be moved to a .tex file on Overleaf for ease of presentation and eventual merging into my paper.
- For now I will work with the CI-R1 experiment. The Contiual ImageNet experiment with 1 pass though the dataset.
- I have implemented this experiment as CI_ViT_V1.py and CI_ViT_V1_resets.py
- So far, all config files are in config/December24 Configs/ViT

TODO META: 
- I need to figure out how and where to log results.
- Master Directory:
- /pool001/jozefiak/CI_ViT/
- ../experiment_name/alg_h-params/seed_i/
- /lr_sweep/ 
- /test_acc.pkl
- /train_acc.pkl
- /train_loss.pkl
- e.g. - /pool001/jozefiak/CI_ViT/lr_sweep/Vanilla_lr_1e-1/seed_0/test_acc.pkl
- TODO: make a function that saves to disk and creates the necessary directory if it does not experiments


TODO 1:
- I will begin by performing a learning rate sweep for 5 seeds.
- I should record the train loss, train accuracy, and train error.
- I could record other covarites, but perhaps that should be done later.
- The other covariates to record are: neuron activities, neuron ages, entropies of self attention layers, weight magnitudes

- I have completed TODO 1.
- For 3 layers, lr = 1e-3 is better initially and has clear plasticity loss while lr = 1e-4 performs better eventually.
- For 12 layers, lr = 1e-4 is clearly better htroughout and shows minimal plasticity train_loss
- For 12 layers, lr = 1e-3 shows clear plasticity loss and is better than lr = 1e-5 initially, which seems to improve over time.

TODO 2: I am now implementing and running the hyperparameter sweeps.
- I am running the sweeps for L2 and L2Init from 1e-1 to 1e-7
- I have just launched a Shrink and Perturb sweep. 
- These leaves me with implementing the reset based algorithms, then the *-algorithms.

TODO 3: Reset Algorithms:
- I have made some progress on refactoring my code for the reset algorithms.
- I need to write a new experiment loop with resets.


- Note: For S&P I did not fix the use of the random number generator and I was using the same key throughout an epoch.
- This may have caused some issues, so I should look into it later potentially. But there are 500*10 epochs, so perhaps this is not too concerning for now.

12/30/24 Update: I am back from my trip. I will begin today with implementing and testing the reset algorithms. Then I will run the massive hyperparameter sweep.
- CI_ViT_V1_resets.py looks good, but I need to run it to be sure.
- get_neuron_ages_ViT.py looks reasonable, but I should double check that this one works.
-- update_neuron_ages, get_neuron_pre_activ CI_ViT_V1_resets appear to be working.
- reset_neurons_ViT.py seems to look correct for CBP, though I really need to test it
-- neuron_ages do not appear to be updating correctly, or at all rather from the reset_neurons function for CBP
-- Actually, it appears that we do not update neuron_ages in CBP

NOTE: So far my crude tests seem to show that CBP is working as expected, I just need to keep in mind the below notes


FUTURE TODOs:

- TODO: I just need to change the configs to account for the number of neurons due to scaling: n_neurons = 4 * hidden_dim
- TODO: For CBP, I may need to lower the age threshold and the perhaps play with the decay factor.
- At the moment, since we are counting inactivity in terms of units of batches, an age threshold of 100 implies that we 
- can only reset a neuron after 100 batches which is almost one task. Perhaps this is not too bad for this application.
- TODO: For CBP, it does not appear that we are updating neuron_ages in the reset_neurons, therefore:
- for all methods except SNR, we should be manually updating neuron_ages when logging all covariates. 


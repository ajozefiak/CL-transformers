12/23/24

- Here I am creating a notes section.
- Some of this will be moved to a .tex file on Overleaf for ease of presentation and eventual merging into my paper.
- For now I will work with the CI-R1 experiment. The Contiual ImageNet experiment with 1 pass though the dataset.
- I have implemented this experiment as CI_ViT_V1.py and CI_ViT_V1_resets.py
- So far, all config files are in config/December24 Configs/ViT

TODO META: 
- I need to figure out how and where to log results.
- Master Directory:
- /pool001/jozefiak/CI_ViT/
- ../experiment_name/alg_h-params/seed_i/
- /lr_sweep/ 
- /test_acc.pkl
- /train_acc.pkl
- /train_loss.pkl
- e.g. - /pool001/jozefiak/CI_ViT/lr_sweep/Vanilla_lr_1e-1/seed_0/test_acc.pkl
- TODO: make a function that saves to disk and creates the necessary directory if it does not experiments


TODO 1:
- I will begin by performing a learning rate sweep for 5 seeds.
- I should record the train loss, train accuracy, and train error.
- I could record other covarites, but perhaps that should be done later.
- The other covariates to record are: neuron activities, neuron ages, entropies of self attention layers, weight magnitudes

- I have completed TODO 1.
- For 3 layers, lr = 1e-3 is better initially and has clear plasticity loss while lr = 1e-4 performs better eventually.
- For 12 layers, lr = 1e-4 is clearly better htroughout and shows minimal plasticity train_loss
- For 12 layers, lr = 1e-3 shows clear plasticity loss and is better than lr = 1e-5 initially, which seems to improve over time.

TODO 2: I am now implementing and running the hyperparameter sweeps.
- I am running the sweeps for L2 and L2Init from 1e-1 to 1e-7
- I have just launched a Shrink and Perturb sweep. 
- These leaves me with implementing the reset based algorithms, then the *-algorithms.

TODO 3: Reset Algorithms:
- I have made some progress on refactoring my code for the reset algorithms.
- I need to write a new experiment loop with resets.


- Note: For S&P I did not fix the use of the random number generator and I was using the same key throughout an epoch.
- This may have caused some issues, so I should look into it later potentially. But there are 500*10 epochs, so perhaps this is not too concerning for now.

12/30/24 Update: I am back from my trip. I will begin today with implementing and testing the reset algorithms. Then I will run the massive hyperparameter sweep.
- CI_ViT_V1_resets.py looks good, but I need to run it to be sure.
- get_neuron_ages_ViT.py looks reasonable, but I should double check that this one works.
-- update_neuron_ages, get_neuron_pre_activ CI_ViT_V1_resets appear to be working.
- reset_neurons_ViT.py seems to look correct for CBP, though I really need to test it
-- neuron_ages do not appear to be updating correctly, or at all rather from the reset_neurons function for CBP
-- Actually, it appears that we do not update neuron_ages in CBP

NOTE: So far my crude tests seem to show that CBP is working as expected, I just need to keep in mind the below notes

Update at 6:20 PM: I have just my CBP sweep for num_layers=3, lr=1e-3, 
- The script is not saving results after each seed like with S&P, which is a bit strange.
- This terminated due a typo, I was using x instead of x_batch, but the warning that I am receiving should be independent of this.
- I am in the process of implementing ReDO and I will try and run ReDO with num_layers=3, lr=1e-3 before running the num_layers=12 experiments.
- The cluster is now much busier.
- ReDO looks good to me, but I have not tested it on the Colab yet.
- I have placed CBP and ReDO jobs on the cluster. They are waiting for priority and resoures.


Update at 6:20 PM of 12/31/24:
- I have ran the CBP sweep, the ReDO sweep has completed for L3 and is in progress for L12
- I have scheduled the SNR sweep. 
- Next, I need to write an experimental loop for running each optimal hyperparameter choice for 5 new seeds and recording the covariates.
- I also need to record the optimal hyperparameters.
- All of this needs to be put into a document, which will be a bit annoying since I need to record all of the experimental details, architectural details and so forth.

Covariates and statistics to record:
- train loss, train accuracy, test accuracy: We are currently doing this.
- Magnitudes of weights: I can record this at the end of each opoch (or task)
- Neuron ages (measures inactivity)
- Neuron resets
- entropies of self attention layers.

- I should implement this and run it for the Vanilla algorithm in Colab to confirm that it works.
- Then I can run this on the cluster or even Colab for every algorithm.


FUTURE TODOs:

- DONE: TODO: I just need to change the configs to account for the number of neurons due to scaling: n_neurons = 4 * hidden_dim
- TODO: For CBP, I may need to lower the age threshold and the perhaps play with the decay factor.
- At the moment, since we are counting inactivity in terms of units of batches, an age threshold of 100 implies that we 
- can only reset a neuron after 100 batches which is almost one task. Perhaps this is not too bad for this application.
- DONE (see below): TODO: For CBP, it does not appear that we are updating neuron_ages in the reset_neurons, therefore:
- DONE: for all methods except SNR, we should be manually updating neuron_ages when logging all covariates. 

1/1/25: 
- Yesterday I have run the SNR algorithms for L3 and L12. The L12 sweeps are still running.
- I have also implemented the CI_ViT_V1_log_correlates.py file which logs all of the above correlates. 
- I have run this for Vanilla L3 with lr 1e-3, and L12 with lr 1e-3 and 1e-4
- I need to now make a notebook on the cluster that goes through each hyperparamter sweep and determines the optimal hyperparameters for each algorithm.

- L2: L3: reg_str = 1e-4, L12 w/ lr = 1e-3: reg_str = 1e-3, L12 w/ lr = 1e-4: reg_str = 1e-5
- L2Init: L3: reg_str = 1e-2, L12 w/ lr = 1e-3: reg_str = 1e-4, L12 w/ lr = 1e-4: reg_str = 1e-2 (last 50) 1e-1 (last 10)

Note S&P results look quite poor for some reason, I may need to debug and figure out what is going on
- S&P: L3  w/ lr = 1e-3: p = 0.99, sigma = 1e-2
- S&P: L12 w/ lr = 1e-3: p = 0.7,  sigma = 1e-1 (last 50), p = 0.99, sigma = 1e-1 (last 10)
- S&P: L12 w/ lr = 1e-4: p = 0.99, sigma = 1e-2, (last 50) p = 0.975, sigma = 1e-2 (last 10)

- ReDO: L3  w/ lr = 1e-3: threshold = 0.08, reset_freq = 0.0166... (last 50), threshold = 0.02, reset_freq = 0.0166... (last 10)
- ReDO: L12 w/ lr = 1e-3: threshold = 0.01, reset_freq = 0.0166... (last 50), threshold = 0.00, reset_freq = 0.002083... (last 10)
- ReDO: L12 w/ lr = 1e-4: threshold = 0.08, reset_freq = 0.0166... (last 50), threshold = 0.08, reset_freq = 0.0166... (last 10)

- CBP: L3: reset_freq = 1e-2, L12 w/ lr = 1e-3: reset_freq = 1e-2, L12 w/ lr = 1e-4: reset_freq = 1e-2 (last 50) 1e-1 (last 10, barely better)

- SNR: 

